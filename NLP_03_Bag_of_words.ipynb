{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_03_Bag_of_words.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNw8A3A1BTJb"
      },
      "source": [
        "Bag of words\n",
        "\n",
        "Steps :\n",
        "\n",
        "Data ----> Clean data ----> Tokenize ----> Build Vocabulary ----> Generate Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xObd3Mu_BnDH"
      },
      "source": [
        "BOW ( Bag of words ) is an approach used with \n",
        "\n",
        "1. NLP \n",
        "2. Information retrieval from documents \n",
        "3. Document classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "394hMSCgB2Zs"
      },
      "source": [
        "Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MkVHhLfBNVz"
      },
      "source": [
        "import numpy\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1yod-aFBSCl"
      },
      "source": [
        "Word Extraction , Tokenize and Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4egH5_DoB_EM",
        "outputId": "43e450e1-568a-4316-918a-0f54a8c8db89"
      },
      "source": [
        "def word_extraction(sentence):\n",
        "  \n",
        "  ignore = [\"a\",\"the\",\"is\"]\n",
        "  words = re.sub(\"[^\\w]\",\" \", sentence).split()\n",
        "  cleaned_text =[w.lower() for w in words if w not in ignore]\n",
        "  return cleaned_text\n",
        "\n",
        "def tokenize(sentences):\n",
        "  \n",
        "  words = []\n",
        "  \n",
        "  for sentence in sentences:\n",
        "    \n",
        "    w = word_extraction(sentence)\n",
        "    words.extend(w)\n",
        "    \n",
        "  \n",
        "  words = sorted(list(set(words)))\n",
        "  return words\n",
        "\n",
        "def generate_bow(allsentences):\n",
        "\n",
        "  vocab = tokenize(allsentences)\n",
        "  print(\"Word list \\n{0}\\n\".format(vocab))\n",
        "\n",
        "  for sentence in allsentences:\n",
        "\n",
        "    words = word_extraction(sentence)\n",
        "    bag_vector = numpy.zeros(len(vocab))\n",
        "\n",
        "    for w in words:\n",
        "\n",
        "      for i, word in enumerate(vocab):\n",
        "\n",
        "        if word == w:\n",
        "\n",
        "          bag_vector[i] += 1\n",
        "      \n",
        "    print(\"{0}\\n{1}\\n\".format(sentence , numpy.array(bag_vector)))\n",
        "\n",
        "allsentences =[\"Joe waited for the train\", \"The train was late\",\"Mary and Samantha took the bus\", \"I looked for Samantha and Mary at the bus station\",\"Mary and Samantha arrived at the bus station early but waited until noon for the bus\"]\n",
        "\n",
        "generate_bow(allsentences)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word list \n",
            "['and', 'arrived', 'at', 'bus', 'but', 'early', 'for', 'i', 'joe', 'late', 'looked', 'mary', 'noon', 'samantha', 'station', 'the', 'took', 'train', 'until', 'waited', 'was']\n",
            "\n",
            "Joe waited for the train\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "\n",
            "The train was late\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
            "\n",
            "Mary and Samantha took the bus\n",
            "[1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "\n",
            "I looked for Samantha and Mary at the bus station\n",
            "[1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "Mary and Samantha arrived at the bus station early but waited until noon for the bus\n",
            "[1. 1. 1. 2. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0.]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Anq5ZVWBHQbV"
      },
      "source": [
        "Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAe8tn-eC_gX",
        "outputId": "61d14667-db5b-4b4a-ec48-5c49ae20ded3"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(allsentences)\n",
        "print(\"All sentences:\\n\\n\")\n",
        "for sentence in allsentences:\n",
        "  print(sentence,\"\\n\\n\")\n",
        "print(\"Matrix of features for each sentence : \\n\\n\",X.toarray())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All sentences:\n",
            "\n",
            "\n",
            "Joe waited for the train \n",
            "\n",
            "\n",
            "The train was late \n",
            "\n",
            "\n",
            "Mary and Samantha took the bus \n",
            "\n",
            "\n",
            "I looked for Samantha and Mary at the bus station \n",
            "\n",
            "\n",
            "Mary and Samantha arrived at the bus station early but waited until noon for the bus \n",
            "\n",
            "\n",
            "Matrix of features for each sentence : \n",
            "\n",
            " [[0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1]\n",
            " [1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0]\n",
            " [1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0]\n",
            " [1 1 1 2 1 1 1 0 0 0 1 1 1 1 2 0 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfiHYFQEIZSc"
      },
      "source": [
        "Bigrams , Trigrams and N grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myAPa-ZpIxP6"
      },
      "source": [
        "Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcMRdjgeHmCN",
        "outputId": "ecc494ae-2b4d-4717-bea0-380a4774f3d2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSvHWVNQJOSm"
      },
      "source": [
        "Code for bigrams of the word \"sun\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1XDpQuVIzwQ",
        "outputId": "3030a88e-44f7-4867-f700-3a77bfac5a4f"
      },
      "source": [
        "word_list = []\n",
        "stops = set(stopwords.words('english'))\n",
        "[word_list.extend(nltk.corpus.gutenberg.words(f)) for f in nltk.corpus.gutenberg.fileids()]\n",
        "cleaned_words = [w.lower() for w in word_list if w.isalnum()]\n",
        "sun_bigrams = [b for b in nltk.bigrams(cleaned_words) if (b[0] == 'sun' or b[1] == 'sun')\\\n",
        "               and b[0] not in stops and b[1] not in stops]\n",
        "print(\"Sets :\\n\",set(sun_bigrams))\n",
        "print(\"\\n\\nLength of total number of combinations found for sun:\\n\",len(set(sun_bigrams)))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sets :\n",
            " {('sun', 'soaked'), ('sun', 'ariseth'), ('ye', 'sun'), ('sun', '12'), ('spangling', 'sun'), ('sun', 'making'), ('sun', '1'), ('blazing', 'sun'), ('sun', 'riseth'), ('lucifer', 'sun'), ('sun', '2'), ('blinding', 'sun'), ('sun', 'shining'), ('sun', 'move'), ('sun', 'moon'), ('sun', 'threw'), ('sun', 'impearls'), ('runaway', 'sun'), ('silent', 'sun'), ('sun', 'toasted'), ('sun', 'shone'), ('sun', 'lit'), ('sun', 'excludes'), ('forenoon', 'sun'), ('great', 'sun'), ('day', 'sun'), ('sun', 'carefully'), ('sun', 'gilds'), ('sun', 'upon'), ('sultry', 'sun'), ('coined', 'sun'), ('haired', 'sun'), ('sheeny', 'sun'), ('rising', 'sun'), ('sun', 'stand'), ('sun', 'gained'), ('sun', 'sinking'), ('sun', 'ahab'), ('sun', 'keep'), ('sun', 'aye'), ('sun', 'go'), ('sun', 'dial'), ('sun', 'rise'), ('sun', 'unto'), ('sun', 'seeking'), ('sun', 'dalroy'), ('sun', 'burnt'), ('sun', 'entering'), ('abated', 'sun'), ('gayety', 'sun'), ('flashing', 'sun'), ('sun', 'measuring'), ('sinking', 'sun'), ('sun', 'saying'), ('modern', 'sun'), ('sun', 'appeared'), ('thy', 'sun'), ('sun', 'like'), ('sun', 'dried'), ('sun', 'climbed'), ('victorious', 'sun'), ('sun', 'hath'), ('bright', 'sun'), ('coming', 'sun'), ('sun', 'bursts'), ('sun', '7'), ('sun', '9'), ('sun', 'ingendered'), ('sun', 'following'), ('needed', 'sun'), ('sun', '16'), ('sun', 'oh'), ('sun', 'toward'), ('sun', 'besides'), ('sun', 'thy'), ('seeing', 'sun'), ('sun', 'glade'), ('glaring', 'sun'), ('shining', 'sun'), ('sun', 'hark'), ('sun', 'moby'), ('sun', 'went'), ('sun', 'stands'), ('sun', 'namely'), ('sun', 'slow'), ('mounted', 'sun'), ('earth', 'sun'), ('sun', 'ho'), ('sun', 'chapter'), ('sun', 'declined'), ('sun', 'goes'), ('sun', 'shineth'), ('sun', 'waxed'), ('sun', 'gave'), ('sun', 'shine'), ('sun', 'seemed'), ('sun', 'evan'), ('sun', 'see'), ('sun', 'thought'), ('sun', 'burning'), ('sun', 'tan'), ('hump', 'sun'), ('sun', '4'), ('sun', 'frequently'), ('sun', 'turnbull'), ('west', 'sun'), ('sun', 'astern'), ('sun', 'embracing'), ('pleasant', 'sun'), ('sun', 'bonnet'), ('sun', 'swept'), ('sun', 'turned'), ('sun', 'falls'), ('sun', 'ever'), ('sun', 'stars'), ('sun', 'beyond'), ('sun', 'slowly'), ('sun', 'hides'), ('immense', 'sun'), ('sun', 'splashed'), ('sun', 'descending'), ('afternoon', 'sun'), ('send', 'sun'), ('sun', 'usher'), ('running', 'sun'), ('sun', 'playing'), ('sun', 'level'), ('sun', 'wears'), ('sun', 'neither'), ('summer', 'sun'), ('sun', 'stood'), ('poor', 'sun'), ('sun', 'another'), ('glad', 'sun'), ('sun', '8'), ('strong', 'sun'), ('invisible', 'sun'), ('commanding', 'sun'), ('white', 'sun'), ('volcanoes', 'sun'), ('clear', 'sun'), ('sun', '19'), ('sun', '58'), ('sun', 'returned'), ('sun', 'animals'), ('hot', 'sun'), ('th', 'sun'), ('sun', 'even'), ('sun', 'failing'), ('sun', 'whether'), ('sun', 'science'), ('sun', 'knoweth'), ('sun', 'smite'), ('sun', 'cast'), ('sun', 'ashamed'), ('sun', 'things'), ('sun', 'shifted'), ('sun', 'come'), ('torrid', 'sun'), ('weaker', 'sun'), ('sun', 'said'), ('sun', 'shines'), ('sun', 'paint'), ('japanese', 'sun'), ('sun', 'soon'), ('sun', 'queen'), ('dazzling', 'sun'), ('sun', 'descried'), ('natural', 'sun'), ('sun', 'producing'), ('excellent', 'sun'), ('upper', 'sun'), ('midnight', 'sun'), ('diver', 'sun'), ('mr', 'sun'), ('risen', 'sun'), ('beaming', 'sun'), ('sun', 'rose'), ('sun', 'beam'), ('sun', 'ran'), ('sun', 'tired'), ('sun', 'wilt'), ('mother', 'sun'), ('golden', 'sun'), ('thou', 'sun'), ('sun', 'light'), ('sun', 'became'), ('low', 'sun'), ('sun', 'breed'), ('sun', 'never'), ('sun', 'would'), ('sun', 'something'), ('morning', 'sun'), ('equatorial', 'sun'), ('sun', 'around'), ('sun', 'wheels'), ('sun', 'meets'), ('sun', 'bright'), ('sun', 'swings'), ('sun', 'afloat'), ('burnished', 'sun'), ('sun', '17'), ('sun', 'shot'), ('setting', 'sun'), ('midday', 'sun'), ('sun', 'half'), ('sun', 'goeth'), ('sun', 'till'), ('neither', 'sun'), ('israel', 'sun'), ('sun', 'shall'), ('sun', 'first'), ('sun', 'two'), ('chemick', 'sun'), ('orient', 'sun'), ('sun', 'beat'), ('keystone', 'sun'), ('red', 'sun'), ('slanting', 'sun'), ('sun', 'predominant'), ('sun', 'entered'), ('downward', 'sun'), ('sun', 'also'), ('sun', 'set'), ('ratifying', 'sun'), ('parting', 'sun'), ('sun', 'action'), ('sun', 'dropt'), ('sun', '74'), ('bearded', 'sun'), ('sun', 'instantly'), ('earliest', 'sun'), ('sun', 'new'), ('sun', 'freighted'), ('radiant', 'sun'), ('western', 'sun'), ('sky', 'sun'), ('sun', '11'), ('sun', 'going'), ('sun', 'radiant'), ('sun', 'seems'), ('sun', 'though'), ('mounting', 'sun'), ('sun', 'burned'), ('quickening', 'sun')}\n",
            "\n",
            "\n",
            "Length of total number of combinations found for sun:\n",
            " 245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5giuerkpNQRf"
      },
      "source": [
        "N grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRYPDx2PJG0t",
        "outputId": "7976324e-cd03-4355-b570-15c47daff4fc"
      },
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "\n",
        "sentences = [\"To Sherlock Holmes she is always the woman.\",\"I have seldom heard him mention her under any other name.\"]\n",
        "bigrams = []\n",
        "\n",
        "for sentence in sentences:\n",
        "  sequence = word_tokenize(sentence)\n",
        "  bigrams.extend(list(ngrams(sequence,2)))\n",
        "\n",
        "freq_dist = nltk.FreqDist(bigrams)\n",
        "prob_dist = nltk.MLEProbDist(freq_dist)\n",
        "number_of_bigrams = freq_dist.N()\n",
        "\n",
        "print(\"\\n\\nAll the bigrams:\\n\\n \",bigrams)\n",
        "print(\"\\n\\n Number of bigrams:\\n\\n\",number_of_bigrams)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\n",
            "\n",
            "All the bigrams:\n",
            "\n",
            "  [('To', 'Sherlock'), ('Sherlock', 'Holmes'), ('Holmes', 'she'), ('she', 'is'), ('is', 'always'), ('always', 'the'), ('the', 'woman'), ('woman', '.'), ('I', 'have'), ('have', 'seldom'), ('seldom', 'heard'), ('heard', 'him'), ('him', 'mention'), ('mention', 'her'), ('her', 'under'), ('under', 'any'), ('any', 'other'), ('other', 'name'), ('name', '.')]\n",
            "\n",
            "\n",
            " Number of bigrams:\n",
            "\n",
            " 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SOPCmttOZyv"
      },
      "source": [
        "Trying trigrams and fourgrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uckWg1VOKq-",
        "outputId": "fb544167-ffe0-4046-a5d4-1ab5e9d4c907"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "text = \"I am aware that nltk only offers bigrams and trigrams, but is there a way to split to 4 grams\"\n",
        "\n",
        "tokenize = nltk.word_tokenize(text)\n",
        "print(\"\\n\\nNormal Tokenization :\\n\",tokenize)\n",
        "print(\"\\nTotal number of tokens :\\n\",len(tokenize))\n",
        "\n",
        "trigrams = ngrams(tokenize,3)\n",
        "\n",
        "fourgrams = ngrams(tokenize,4)\n",
        "\n",
        "def get_ngrams(n_grams):\n",
        "  return [\" \".join(grams) for grams in n_grams]\n",
        "\n",
        "trigram = get_ngrams(trigrams)\n",
        "print(\"\\n\\n Trigrams :\\n\",trigram)\n",
        "print(\"\\n No of tokens :\\n\",len(trigram))\n",
        "\n",
        "fourgram = get_ngrams(fourgrams)\n",
        "print(\"\\n\\n Fourgrams :\\n\",fourgram)\n",
        "print(\"\\n No of tokens :\\n\",len(fourgram))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Normal Tokenization :\n",
            " ['I', 'am', 'aware', 'that', 'nltk', 'only', 'offers', 'bigrams', 'and', 'trigrams', ',', 'but', 'is', 'there', 'a', 'way', 'to', 'split', 'to', '4', 'grams']\n",
            "\n",
            "Total number of tokens :\n",
            " 21\n",
            "\n",
            "\n",
            " Trigrams :\n",
            " ['I am aware', 'am aware that', 'aware that nltk', 'that nltk only', 'nltk only offers', 'only offers bigrams', 'offers bigrams and', 'bigrams and trigrams', 'and trigrams ,', 'trigrams , but', ', but is', 'but is there', 'is there a', 'there a way', 'a way to', 'way to split', 'to split to', 'split to 4', 'to 4 grams']\n",
            "\n",
            " No of tokens :\n",
            " 19\n",
            "\n",
            "\n",
            " Fourgrams :\n",
            " ['I am aware that', 'am aware that nltk', 'aware that nltk only', 'that nltk only offers', 'nltk only offers bigrams', 'only offers bigrams and', 'offers bigrams and trigrams', 'bigrams and trigrams ,', 'and trigrams , but', 'trigrams , but is', ', but is there', 'but is there a', 'is there a way', 'there a way to', 'a way to split', 'way to split to', 'to split to 4', 'split to 4 grams']\n",
            "\n",
            " No of tokens :\n",
            " 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye0SMDDePLBV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}